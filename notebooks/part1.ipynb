{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Named Entity Recognition for Healthcare\n",
    "\n",
    "What is Named Entity Recognition.\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as the names of persons, organizations, locations, medical terms, etc. In healthcare, NER can be used to extract relevant medical entities from clinical notes, research papers, and other medical documents."
   ],
   "id": "1b6302e08fc4c9cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## AIM and Objectives\n",
    "The aim of this project is to develop a Named Entity Recognition (NER) system specifically tailored for healthcare applications. The objectives include:\n",
    " - Developing a robust NER model that can accurately identify and classify medical entities in clinical texts.\n",
    "  - Evaluating the model's performance using standard metrics such as precision, recall, and F1-score.\n",
    "  - Exploring the use of pre-trained language models and transfer learning techniques to improve NER performance."
   ],
   "id": "4f12b727ce81f4cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Related Work\n",
    "In healthcare, NER has been applied to various tasks such as extracting drug names, medical conditions, and treatment plans from clinical notes. Previous studies have shown that using domain-specific language models can significantly improve NER performance in healthcare contexts. For instance, models like BioBERT and ClinicalBERT have been fine-tuned on large biomedical corpora to enhance their understanding of medical terminology and context.\n",
    "Example of these models include:\n",
    "- BioBERT: A pre-trained biomedical language representation model based on BERT.\n",
    "- ClinicalBERT: A variant of BERT fine-tuned on clinical notes to improve performance on healthcare-related tasks.\n",
    "- Med7: A transformer-based model specifically designed for NER in the medical domain, achieving state-of-the-art results on various biomedical NER benchmarks."
   ],
   "id": "be260301b40cd806"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Datasets\n",
    "\n",
    "The datasets used for training and evaluating the NER model include:\n",
    "- BC5CDR: A large corpus of clinical notes annotated with medical entities, including diseases, treatments, and medications.\n",
    "- NCBI Disease Corpus: A collection of biomedical literature annotated with disease entities, providing a rich source of medical terminology and context.\n",
    "- MedMentions: A dataset containing mentions of medical concepts in clinical texts, annotated with their corresponding UMLS (Unified Medical Language System) concepts.\n",
    "\n",
    "> Some other datasets that can be used for NER in healthcare and will be listed here in the future"
   ],
   "id": "118afa93dc2a65da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Methodology",
   "id": "f6969933fcc3c0b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "cf6ecd382b904bf8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-22T18:32:54.260487Z",
     "start_time": "2025-07-22T18:32:48.777935Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "# Load the BC5CDR dataset\n",
    "dataset = load_dataset('tner/bc5cdr')\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35825a767f6c4e1297a22a25795072dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bc5cdr.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40905e7ce9464c2e9264c78ce3b4fd6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/367k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7a0fe108db244dc8e1f322945a7aad4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/364k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3928b5f741cb4f11a7ad1318a918a2d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/386k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9bc1caedaad4456ba3df93253357691"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/5228 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a46de84d2bfe4d3a9c0abf382f1f518d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/5330 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ffc3bdfaf4544c781014f77b5edbb2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/5865 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c71ed14126dd45888ad259021fceb81c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5228\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5330\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5865\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The dataset has 3 splits: 'train', 'validation', and 'test'. Each split contains text data along with the corresponding entity annotations.",
   "id": "ab88c61ec642479d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:35:15.316945Z",
     "start_time": "2025-07-22T18:35:15.312570Z"
    }
   },
   "cell_type": "code",
   "source": "len(dataset['train']), len(dataset['validation']), len(dataset['test'])",
   "id": "ade1f6be1b0d67c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5228, 5330, 5865)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T18:36:01.933770Z",
     "start_time": "2025-07-22T18:36:01.928872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "random.seed(0)  # Set a random seed for reproducibility\n",
    "dataset['train'][random.randint(0, len(dataset['train']))] # Load any example from the training set"
   ],
   "id": "8a7f6c1b9b4b414b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['RESULTS',\n",
       "  ':',\n",
       "  'All',\n",
       "  'the',\n",
       "  'patients',\n",
       "  'were',\n",
       "  'examined',\n",
       "  'for',\n",
       "  'toxicity',\n",
       "  ';',\n",
       "  '34',\n",
       "  'were',\n",
       "  'examinable',\n",
       "  'for',\n",
       "  'response',\n",
       "  '.'],\n",
       " 'tags': [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Selection and Training",
   "id": "d47ee7544df47a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "79203dc707d04d96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
