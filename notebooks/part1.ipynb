{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Named Entity Recognition for Healthcare\n",
    "\n",
    "What is Named Entity Recognition.\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as the names of persons, organizations, locations, medical terms, etc. In healthcare, NER can be used to extract relevant medical entities from clinical notes, research papers, and other medical documents."
   ],
   "id": "1b6302e08fc4c9cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## AIM and Objectives\n",
    "The aim of this project is to develop a Named Entity Recognition (NER) system specifically tailored for healthcare applications. The objectives include:\n",
    " - Developing a robust NER model that can accurately identify and classify medical entities in clinical texts.\n",
    "  - Evaluating the model's performance using standard metrics such as precision, recall, and F1-score.\n",
    "  - Exploring the use of pre-trained language models and transfer learning techniques to improve NER performance."
   ],
   "id": "4f12b727ce81f4cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Related Work\n",
    "In healthcare, NER has been applied to various tasks such as extracting drug names, medical conditions, and treatment plans from clinical notes. Previous studies have shown that using domain-specific language models can significantly improve NER performance in healthcare contexts. For instance, models like BioBERT and ClinicalBERT have been fine-tuned on large biomedical corpora to enhance their understanding of medical terminology and context.\n",
    "Example of these models include:\n",
    "- BioBERT: A pre-trained biomedical language representation model based on BERT.\n",
    "- ClinicalBERT: A variant of BERT fine-tuned on clinical notes to improve performance on healthcare-related tasks.\n",
    "- Med7: A transformer-based model specifically designed for NER in the medical domain, achieving state-of-the-art results on various biomedical NER benchmarks."
   ],
   "id": "be260301b40cd806"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Datasets\n",
    "\n",
    "The datasets used for training and evaluating the NER model include:\n",
    "- BC5CDR: A large corpus of clinical notes annotated with medical entities, including diseases, treatments, and medications.\n",
    "- NCBI Disease Corpus: A collection of biomedical literature annotated with disease entities, providing a rich source of medical terminology and context.\n",
    "- MedMentions: A dataset containing mentions of medical concepts in clinical texts, annotated with their corresponding UMLS (Unified Medical Language System) concepts.\n",
    "\n",
    "> Some other datasets that can be used for NER in healthcare and will be listed here in the future"
   ],
   "id": "118afa93dc2a65da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Methodology",
   "id": "f6969933fcc3c0b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "cf6ecd382b904bf8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. The dataset used has the following labels:\n",
    "   - `O`: 0\n",
    "   - `B-Chemical`: 1\n",
    "   - `B-Disease`: 2\n",
    "   - `I-Disease`: 3\n",
    "   - `I-Chemical`: 4\n",
    "\n",
    "\n",
    "\n",
    "> We will use another dataset apart from this one in the future, but for now we will use the BC5CDR dataset."
   ],
   "id": "87369fa7718ab0d6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-22T20:48:27.887853Z",
     "start_time": "2025-07-22T20:48:26.319831Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the BC5CDR dataset\n",
    "dataset = load_dataset('tner/bc5cdr')\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5228\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5330\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 5865\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The dataset has 3 splits: 'train', 'validation', and 'test'. Each split contains text data along with the corresponding entity annotations.",
   "id": "ab88c61ec642479d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:32:00.877277Z",
     "start_time": "2025-07-22T20:32:00.874373Z"
    }
   },
   "cell_type": "code",
   "source": "len(dataset['train']), len(dataset['validation']), len(dataset['test'])",
   "id": "ade1f6be1b0d67c5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5228, 5330, 5865)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:32:02.282289Z",
     "start_time": "2025-07-22T20:32:02.277111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "random.seed(0)  # Set a random seed for reproducibility\n",
    "dataset['train'][random.randint(0, len(dataset['train']))] # Load any example from the training set"
   ],
   "id": "8a7f6c1b9b4b414b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['RESULTS',\n",
       "  ':',\n",
       "  'All',\n",
       "  'the',\n",
       "  'patients',\n",
       "  'were',\n",
       "  'examined',\n",
       "  'for',\n",
       "  'toxicity',\n",
       "  ';',\n",
       "  '34',\n",
       "  'were',\n",
       "  'examinable',\n",
       "  'for',\n",
       "  'response',\n",
       "  '.'],\n",
       " 'tags': [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b1fb043d3a28f24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Selection and Training",
   "id": "d47ee7544df47a75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:32:10.058905Z",
     "start_time": "2025-07-22T20:32:07.160789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label = {0: \"O\", 1: \"B-Chemical\", 2: \"B-Disease\", 3: \"I-Disease\", 4: \"I-Chemical\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# Load a pre-trained tokenizer and model for token classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=5,  # Number of labels in the dataset\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.to(device)"
   ],
   "id": "dc224f8eb96f8d54",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:32:28.472490Z",
     "start_time": "2025-07-22T20:32:27.936126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Add this\n",
    "        max_length=128,        # Or another suitable value\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "# Tokenize the dataset and align labels with tokens\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_datasets['train'][0]"
   ],
   "id": "dab7846cc25eb30f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5330 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a4b86513b88444eb19e19883b5d81c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Naloxone',\n",
       "  'reverses',\n",
       "  'the',\n",
       "  'antihypertensive',\n",
       "  'effect',\n",
       "  'of',\n",
       "  'clonidine',\n",
       "  '.'],\n",
       " 'tags': [1, 0, 0, 0, 0, 0, 1, 0],\n",
       " 'input_ids': [101,\n",
       "  11896,\n",
       "  2858,\n",
       "  21501,\n",
       "  1162,\n",
       "  7936,\n",
       "  1116,\n",
       "  1103,\n",
       "  2848,\n",
       "  7889,\n",
       "  17786,\n",
       "  5026,\n",
       "  2109,\n",
       "  2629,\n",
       "  1104,\n",
       "  172,\n",
       "  4934,\n",
       "  2386,\n",
       "  2042,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'offset_mapping': [[0, 0],\n",
       "  [0, 2],\n",
       "  [2, 4],\n",
       "  [4, 7],\n",
       "  [7, 8],\n",
       "  [0, 7],\n",
       "  [7, 8],\n",
       "  [0, 3],\n",
       "  [0, 4],\n",
       "  [4, 6],\n",
       "  [6, 10],\n",
       "  [10, 13],\n",
       "  [13, 16],\n",
       "  [0, 6],\n",
       "  [0, 2],\n",
       "  [0, 1],\n",
       "  [1, 4],\n",
       "  [4, 6],\n",
       "  [6, 9],\n",
       "  [0, 1],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0]],\n",
       " 'labels': [-100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:33:13.218298Z",
     "start_time": "2025-07-22T20:33:12.377287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    # Multiply only float values by 100\n",
    "    def scale_metrics(d):\n",
    "        return {k: (v * 100 if isinstance(v, float) else v) for k, v in d.items()}\n",
    "    return {k: scale_metrics(v) if isinstance(v, dict) else (v * 100 if isinstance(v, float) else v)\n",
    "            for k, v in results.items()}\n"
   ],
   "id": "292c580f96b5e58b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:43:39.685694Z",
     "start_time": "2025-07-22T20:33:23.415422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args  = TrainingArguments(\n",
    "    output_dir=\"ner-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "b8f8ab1b2e363ae4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/tr2332417vjbmj74tfc0dcd40000gn/T/ipykernel_15303/3949926226.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/anaconda3/envs/Dissertation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='981' max='981' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [981/981 10:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Dissertation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/Dissertation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=981, training_loss=0.13031390051107766, metrics={'train_runtime': 616.1656, 'train_samples_per_second': 25.454, 'train_steps_per_second': 1.592, 'total_flos': 1024572371696640.0, 'train_loss': 0.13031390051107766, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:44:41.684456Z",
     "start_time": "2025-07-22T20:43:44.971339Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.evaluate()",
   "id": "aaa607f33ad23e5c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Dissertation/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334' max='334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [334/334 00:55]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.18123093247413635,\n",
       " 'eval_Chemical': {'precision': 92.13810110974106,\n",
       "  'recall': 93.84136233485708,\n",
       "  'f1': 92.98193220845154,\n",
       "  'number': 19907},\n",
       " 'eval_Disease': {'precision': 81.11102620921525,\n",
       "  'recall': 84.66937863922789,\n",
       "  'f1': 82.85201373712145,\n",
       "  'number': 12537},\n",
       " 'eval_overall_precision': 87.81248126611115,\n",
       " 'eval_overall_recall': 90.29712735790902,\n",
       " 'eval_overall_f1': 89.03747378658481,\n",
       " 'eval_overall_accuracy': 95.08518333908465,\n",
       " 'eval_runtime': 56.7029,\n",
       " 'eval_samples_per_second': 93.999,\n",
       " 'eval_steps_per_second': 5.89,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:45:50.629506Z",
     "start_time": "2025-07-22T20:45:50.228105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained('ner-healthcare-part1')\n",
    "tokenizer.save_pretrained('ner-healthcare-part1')"
   ],
   "id": "e8615a4bccef520a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner-healthcare-part1/tokenizer_config.json',\n",
       " 'ner-healthcare-part1/special_tokens_map.json',\n",
       " 'ner-healthcare-part1/vocab.txt',\n",
       " 'ner-healthcare-part1/added_tokens.json',\n",
       " 'ner-healthcare-part1/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:46:45.365970Z",
     "start_time": "2025-07-22T20:46:44.787867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = 'ner-healthcare-part1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "sentence = \"The patient was diagnosed with diabetes and prescribed metformin.\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ],
   "id": "2f3c4511ecce8c8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 5.4552, -0.9119, -0.3546, -1.7974, -0.9001],\n",
       "         [ 8.7544, -2.2196, -1.4454, -1.4424, -1.5638],\n",
       "         [ 8.7365, -2.2940, -1.1467, -1.1861, -1.6643],\n",
       "         [ 8.9532, -2.1271, -1.4135, -1.5300, -1.6134],\n",
       "         [ 8.1951, -2.3656, -0.5390, -1.4019, -1.8728],\n",
       "         [ 7.6669, -2.0340,  0.1609, -1.4354, -2.2945],\n",
       "         [-0.7699, -1.1184,  5.2833, -0.0622, -2.5795],\n",
       "         [ 8.4840, -1.6472, -1.7583, -1.6701, -1.4407],\n",
       "         [ 8.2167, -1.0587, -1.5711, -1.8173, -1.5328],\n",
       "         [-1.3135,  6.8937, -2.1130, -2.3221, -1.6221],\n",
       "         [-0.9988,  6.8142, -2.2059, -2.4825, -1.5235],\n",
       "         [-0.9092,  6.9303, -2.2710, -2.4274, -1.0259],\n",
       "         [ 8.2220, -1.3677, -1.8447, -1.7283, -1.4924],\n",
       "         [ 4.6525, -0.0528, -1.8846, -1.7758, -0.2108]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:47:21.316803Z",
     "start_time": "2025-07-22T20:47:21.309242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = outputs.logits\n",
    "predicted_label = torch.argmax(logits, dim=-1)\n",
    "predicted_label"
   ],
   "id": "e464f6bd19d1aef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T20:47:59.549775Z",
     "start_time": "2025-07-22T20:47:59.547035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "predictions = predicted_label[0].tolist()\n",
    "for token, prediction in zip(tokens, predictions):\n",
    "    print(f\"{token}: {id2label[prediction]}\")"
   ],
   "id": "41f6ed2123fee9a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: O\n",
      "The: O\n",
      "patient: O\n",
      "was: O\n",
      "diagnosed: O\n",
      "with: O\n",
      "diabetes: B-Disease\n",
      "and: O\n",
      "prescribed: O\n",
      "met: B-Chemical\n",
      "##form: B-Chemical\n",
      "##in: B-Chemical\n",
      ".: O\n",
      "[SEP]: O\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "79203dc707d04d96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
